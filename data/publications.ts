export interface Publication {
  id: string;
  title: string;
  authors: string[];
  venue: string;
  year: number;
  arxiv?: string;
  pdf?: string;
  code?: string;
  project?: string;
  abstract?: string;
  selected?: boolean;
  tldr?: string;
}

export const publications: Publication[] = [
  {
    id: "wang2025promptbridge",
    title: "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
    authors: ["Yaxuan Wang", "Quan Liu", "Zhenting Wang", "Zichao Li", "Wei Wei", "Yang Liu", "Yujia Bao"],
    venue: "arXiv:2512.01420",
    year: 2025,
    arxiv: "2512.01420",
    tldr: "PromptBridge introduces a training-free framework that learns cross-model prompt mappings to preserve performance when switching between different Large Language Models.",
  },
  {
    id: "yang2025webdart",
    title: "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks",
    authors: ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"],
    venue: "arXiv:2510.06587",
    year: 2025,
    arxiv: "2510.06587",
    code: "https://github.com/UCSB-NLP-Chang/WebDART",
    tldr: "WebDART enables LLM agents to handle complex web tasks by dynamically decomposing objectives into navigation, extraction, and execution subtasks with continuous replanning.",
  },
  {
    id: "wang2025mcp",
    title: "MCP-Bench: Benchmarking tool-using llm agents with complex real-world tasks via mcp servers",
    authors: ["Zhenting Wang", "Qi Chang", "Hemani Patel", "Shashank Biju", "Cheng-En Wu", "Quan Liu", "Aolin Ding", "Alireza Rezazadeh", "Ankit Shah", "Yujia Bao", "Eugene Siow"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2026,
    arxiv: "2508.20453",
    code: "https://github.com/Accenture/mcp-bench",
    tldr: "MCP-Bench establishes a benchmark for evaluating LLM agents on realistic multi-step tasks involving tool use and coordination via the Model Context Protocol.",
  },
  {
    id: "kim2025sft",
    title: "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models",
    authors: ["Gyuhak Kim", "Sumiran Singh Thakur", "Su Min Park", "Wei Wei", "Yujia Bao"],
    venue: "arXiv:2506.15021",
    year: 2025,
    arxiv: "2506.15021",
    tldr: "SFT-GO improves Large Language Model alignment by grouping tokens based on importance and optimizing with a weighted combination of worst-group and standard loss.",
  },
  {
    id: "rezazadeh2025collaborative",
    title: "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control",
    authors: ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"],
    venue: "arXiv:2505.18279",
    year: 2025,
    arxiv: "2505.18279",
    tldr: "This work introduces a framework for multi-user, multi-agent collaborative memory with asymmetric, time-evolving access controls to ensure safe knowledge sharing.",
  },
  {
    id: "wu2025advertising",
    title: "Advertising in AI systems: Society must be vigilant",
    authors: ["Menghua Wu", "Yujia Bao"],
    venue: "arXiv:2505.18425",
    year: 2025,
    arxiv: "2505.18425",
    tldr: "This position paper proposes design principles and user strategies for identifying and mitigating commercial bias in generative AI outputs.",
  },
  {
    id: "faltings2025enhancing",
    title: "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
    authors: ["Felix Faltings", "Wei Wei", "Yujia Bao"],
    venue: "Annual Meeting of the Association for Computational Linguistics (ACL)",
    year: 2025,
    arxiv: "2503.17860",
    tldr: "This method enhances retrieval systems by extracting logical structures from queries and composing cosine similarity scores to handle complex logical reasoning at inference time.",
  },
  {
    id: "yang2025kvlink",
    title: "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
    authors: ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Yujia Bao", "Shiyu Chang"],
    venue: "Conference on Neural Information Processing Systems (NeurIPS)",
    year: 2025,
    arxiv: "2502.16002",
    code: "https://github.com/UCSB-NLP-Chang/KVLink",
    tldr: "KVLink accelerates LLM inference and improves accuracy by precomputing and efficiently reusing Key-Value caches for retrieved documents.",
  },
  {
    id: "kuo2025h",
    title: "H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models",
    authors: ["Martin Kuo", "Jianyi Zhang", "Aolin Ding", "Qinsi Wang", "Louis DiValentin", "Yujia Bao", "Wei Wei", "Hai Li", "Yiran Chen"],
    venue: "arXiv:2502.12893",
    year: 2025,
    arxiv: "2502.12893",
    code: "https://github.com/dukeceicenter/jailbreak-reasoning-openai-o1o3-deepseek-r1",
    tldr: "H-CoT demonstrates how attackers can hijack the chain-of-thought reasoning mechanism in Large Reasoning Models to bypass safety guardrails.",
  },
  {
    id: "wang2025dragon",
    title: "DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning",
    authors: ["Yaxuan Wang", "Chris Yuhao Liu", "Quan Liu", "Jinglong Pang", "Wei Wei", "Yujia Bao", "Yang Liu"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2026,
    arxiv: "2511.05784",
    tldr: "DRAGON is a reasoning-based framework that uses in-context chain-of-thought to detect and unlearn harmful knowledge in deployed LLMs without requiring retain data.",
  },
  {
    id: "pang2024improving",
    title: "Improving Data Efficiency via Curating LLM-Driven Rating Systems",
    authors: ["Jinlong Pang", "Jiaheng Wei", "Ankit Parag Shah", "Zhaowei Zhu", "Yaxuan Wang", "Chen Qian", "Yang Liu", "Yujia Bao", "Wei Wei"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2025,
    arxiv: "2410.10877",
    code: "https://github.com/UCSC-REAL/DS2",
    tldr: "DS2 improves data efficiency for instruction tuning by using a diversity-aware score curation method to correct biases in LLM-based rating systems.",
  },
  {
    id: "wang2024llm",
    title: "LLM Unlearning via Loss Adjustment with Only Forget Data",
    authors: ["Yaxuan Wang", "Jiaheng Wei", "Chris Yuhao Liu", "Jinlong Pang", "Quan Liu", "Ankit Parag Shah", "Yujia Bao", "Yang Liu", "Wei Wei"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2025,
    arxiv: "2410.11143",
    code: "https://github.com/UCSC-REAL/FLAT",
    tldr: "FLAT enables LLM unlearning using only forget data by maximizing the divergence between the forget response and a template answer via loss adjustment.",
  },
  {
    id: "rezazadeh2024isolated",
    title: "From isolated conversations to hierarchical schemas: Dynamic tree memory representation for llms",
    authors: ["Alireza Rezazadeh", "Zichao Li", "Wei Wei", "Yujia Bao"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2025,
    arxiv: "2410.14052",
    tldr: "MemTree optimizes long-term memory in LLMs using a dynamic, hierarchical tree structure that organizes information by abstraction level.",
  },
  {
    id: "wu2024sample",
    title: "Sample, estimate, aggregate: A recipe for causal discovery foundation models",
    authors: ["Menghua Wu", "Yujia Bao", "Regina Barzilay", "Tommi Jaakkola"],
    venue: "Transactions on Machine Learning Research (TMLR)",
    year: 2025,
    arxiv: "2402.01929",
    code: "https://github.com/rmwu/sea-reproduce",
    tldr: "This paper proposes a foundation model approach for causal discovery that predicts causal graphs from summary statistics, offering robustness to misspecification.",
  },
  {
    id: "bao2024harnessing",
    title: "Harnessing business and media insights with large language models",
    authors: ["Yujia Bao", "Ankit Parag Shah", "Neeru Narang", "Jonathan Rivers", "Rajeev Maksey", "Lan Guan", "Louise N Barrere", "Shelley Evenson", "Rahul Basole", "Connie Miao", "others"],
    venue: "arXiv:2406.06559",
    year: 2024,
    arxiv: "2406.06559",
    tldr: "Fortune Analytics Language Model (FALM) leverages a curated knowledge base and time-aware reasoning to provide accurate business insights and data visualization.",
  },
  {
    id: "bao2023channel",
    title: "Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words",
    authors: ["Yujia Bao", "Srinivasan Sivanandan", "Theofanis Karaletsos"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2024,
    arxiv: "2309.16108",
    code: "https://github.com/insitro/ChannelViT",
    tldr: "ChannelViT enhances Vision Transformers for multi-channel imaging by constructing patch tokens independently per channel and using hierarchical channel sampling.",
  },
  {
    id: "bao2023contextual",
    title: "Contextual Vision Transformers for Robust Representation Learning",
    authors: ["Yujia Bao", "Theofanis Karaletsos"],
    venue: "arXiv:2305.19402",
    year: 2023,
    arxiv: "2305.19402",
    selected: true,
    code: "https://github.com/insitro/ContextViT",
    tldr: "ContextViT introduces a context token to encapsulate group-specific information, allowing Vision Transformers to adapt to distribution shifts at inference time.",
  },
  {
    id: "bao2022learning",
    title: "Learning to Split for Automatic Bias Detection",
    authors: ["Yujia Bao", "Regina Barzilay"],
    venue: "arXiv:2204.13749",
    year: 2022,
    arxiv: "2204.13749",
    code: "https://github.com/YujiaBao/ls",
    selected: true,
    tldr: "Learning to Split (ls) automatically detects dataset biases by identifying data splits where models trained on one split generalize poorly to the other.",
  },
  {
    id: "bao2021learning",
    title: "Learning Stable Classifiers by Transferring Unstable Features",
    authors: ["Yujia Bao", "Shiyu Chang", "Regina Barzilay"],
    venue: "International Conference on Machine Learning (ICML)",
    year: 2022,
    arxiv: "2106.07847",
    code: "https://github.com/YujiaBao/Tofu",
    selected: true,
    tldr: "This work regularizes target classifiers in transfer learning by explicitly modeling and transferring unstable features from the source domain to maintain robustness.",
  },
  {
    id: "bao2021predict",
    title: "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers",
    authors: ["Yujia Bao", "Shiyu Chang", "Regina Barzilay"],
    venue: "International Conference on Machine Learning (ICML)",
    year: 2021,
    arxiv: "2105.12628",
    code: "https://github.com/YujiaBao/Predict-then-Interpolate",
    selected: true,
    tldr: "Predict then Interpolate (PI) learns stable classifiers by interpolating between correct and incorrect prediction distributions to uncover invariant correlations.",
  },
  {
    id: "bao2020fewshot",
    title: "Few-shot Text Classification with Distributional Signatures",
    authors: ["Yujia Bao*", "Menghua Wu*", "Shiyu Chang", "Regina Barzilay"],
    venue: "International Conference on Learning Representations (ICLR)",
    year: 2020,
    arxiv: "1908.06039",
    code: "https://github.com/YujiaBao/Distributional-Signatures",
    selected: true,
    tldr: "This method improves few-shot text classification by weighting lexical representations with distributional signatures that encode word occurrence patterns.",
  },
  {
    id: "r2a",
    title: "Deriving Machine Attention from Human Rationales",
    authors: ["Yujia Bao", "Shiyu Chang", "Mo Yu", "Regina Barzilay"],
    venue: "Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year: 2018,
    arxiv: "1808.09367",
    code: "https://github.com/YujiaBao/R2A",
    selected: true,
    tldr: "This paper enhances model interpretability and performance in low-resource settings by mapping human rationales to continuous machine attention.",
  },
];
